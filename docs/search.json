[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "index.html#topics-covered",
    "href": "index.html#topics-covered",
    "title": "Overview",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nIntroduction to Time Series Data.\nTime Series Analysis and Visualization.\nForecasting with ARIMA models\nForecasting with ETS models\nAdvanced Concepts: An overview to hierarchical models, deep learning models (eg LSTM) and Large Language Models (LLMs)"
  },
  {
    "objectID": "index.html#resource-faculty",
    "href": "index.html#resource-faculty",
    "title": "Overview",
    "section": "Resource Faculty",
    "text": "Resource Faculty\n\nThe resource faculty for the workshop are experienced professionals with expertise in spatial, time-series and spatiotemporal epidemiology, disease modelling, data science, health research, and public health practice.\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nDr. Arun Kumar Yadav\nProfessor and Commanding Officer,  Armed Forces Medical Services,  India\n\n\n\n\n\n\nBrief Bio\n\n\n\n\n\n\nDr. Arun Kumar Yadav ex Professor of Dept of Community Medicine, AFMC, Pune has guided four MD students and advised more than 1000 postgraduate/undergraduate researchers as secretary of the scientific committee and member secretary of ethics committee. He established a Multi-disciplinary unit (DHR-ICMR), GIS lab, and Health and demographic surveillance system in AFMC, Pune. He provided technical guidance for establishment of wellness center in AFMC, Pune. He is a recipient of three fellowship ( IPHA, IAPSM, AIPI) and members of 11 National and international organization. Currently he is holding the post of Joint secretary (West) in Indian public health association and academic editor of PLOS global health, earlier he was governing council member in IAPSM and executive editor of Indian Journal of Community Medicine. He has published more than 200 peer reviewed research articles in national and international journals (https://www.researchgate.net/profile/Arun-Yadav-12) and is reviewer for Nature medicine, PLOS one, MRC UK, IJPH and other reputed journals. He was assistant editor of WHO textbook of Community Medicine and public health. He is a member of national advisory board for prevention of COVID-19, implementation research and Health system reviews. He was part of team which piloted WHO tool for benchmarking Ethics oversight of Health-related research with human participants in India. He was a member of 15th common review mission for undertaking rapid assessment of functional status of national program under National health mission. He has also been regularly taking interactive session on epidemiology for National board of Examination and has conducted national workshops on systematic reviews, regression, STATA and ‘R’. Currently, he is posted in High altitude area, where he is leading projects for prevention of High-altitude illnesses. He has a special interest in evidence generation including Systematic review and meta-analysis, Hierarchical Modelling, stroke epidemiology, Health technology assessment, implementation research and ethics.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nDr. Gurpreet Singh\nAssociate Professor & Senior Medical Officer,  Armed Forces Medical Services,  India\n\n\n\n\n\n\nBrief Bio\n\n\n\n\n\n\nDr. Gurpreet Singh is a physician, Epidemiologist, and Senior Medical Officer. He holds PhD in Health Data Science from Sree Chitra Tirunal Institute for Medical Sciences and Technology, Trivandrum . Dr Gurpreet completed his MBBS and MD (Community Medicine) from Armed Forces Medical College, Pune in 2007 and 2015 respectively, DNB (Preventive and Social Medicine) from National Board of Examinations, India in 2016. He is recipient of Director General Armed Forces Medical Services Gold Medal, President’s NBE Gold Medal, Certificate of Merit from Maharashtra University of Health Sciences, General Officer Commanding Medallion, Best PhD paper award (IPHACON 2022), and Gavin Mooney Prize (IHEPA). He has contributed chapters to a few books and has 79 scientific publications. His PhD was focused on ”Data Science approach for Spatio-temporal modelling of Dengue in Punjab, India”. He has been an active member of National and International Associations and have contributed immensely towards evacuation of students from Ukraine, relief missions in Syria, and HADR operations during Turkiye earthquake. His area of interest includes Data Science, Epidemiology, Infectious Diseases, Outbreak control and Immunization among others. He is also a firm believer of reproducible science and has conducted multiple cohorts of workshops on introduction to R and R for Spatial Epidemiology."
  },
  {
    "objectID": "4_Model.html",
    "href": "4_Model.html",
    "title": "Workflow: Modeltime",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "4_Model.html#time-series-forecasting-example",
    "href": "4_Model.html#time-series-forecasting-example",
    "title": "Workflow: Modeltime",
    "section": "1 Time Series Forecasting Example",
    "text": "1 Time Series Forecasting Example\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a forecasting exercise, though are multiple ways, we recommend to follow the modeltime workflow, which is detailed in 6 convenient steps:\n\nCollect data and split into training and test sets\n\nCreate & Fit Multiple Models\n\nAdd fitted models to a Model Table\n\nCalibrate the models to a testing set.\n\nPerform Testing Set Forecast & Accuracy Evaluation\n\nRefit the models to Full Dataset & Forecast Forward\n\n\n\nLoad libraries\n\nlibrary(xgboost)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(timetk)\n\n\n1.1 Step 1 - Collect data and split into training and test sets.\n\n\n\nm750 &lt;- m4_monthly |&gt; filter(id == \"M750\")\n\n\nWe can visualize the dataset.\n\nm750 |&gt;\n  plot_time_series(date, value)\n\n\n\n\n\n\n\nLet’s split the data into training and test sets using initial_time_split()\n\n# Split Data 80/20\nsplits &lt;- initial_time_split(m750, prop = 0.9)\n\n\n\n1.2 Step 2 - Create & Fit Multiple Models\nWe can easily create dozens of forecasting models by combining modeltime and parsnip. We can also use the workflows interface for adding preprocessing! Your forecasting possibilities are endless. Let’s get a few basic models developed:\n\nARIMA\nExponential Smoothing\nLinear Regression\nMARS (Multivariate Adaptive Regression Splines)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHandling Date Features\nModeltime models (e.g. arima_reg()) are created with a date or date time feature in the model. You will see that most models include a formula like fit(value ~ date, data).\nParsnip models (e.g. linear_reg()) typically should not have date features, but may contain derivatives of dates (e.g. month, year, etc). You will often see formulas like fit(value ~ as.numeric(date) + month(date), data).\n\n\n\n1.2.1 Model 1: Auto ARIMA (Modeltime)\nFirst, we create a basic univariate ARIMA model using “Auto Arima” using arima_reg()\n\nmodel_fit_arima_no_boost &lt;- arima_reg() |&gt;\n    set_engine(engine = \"auto_arima\") |&gt;\n    fit(value ~ date, \n        data = training(splits))\n\nfrequency = 12 observations per 1 year\n\n\n\n\n1.2.2 Model 2: Boosted Auto ARIMA (Modeltime)\nNext, we create a boosted ARIMA using arima_boost(). Boosting uses XGBoost to model the ARIMA errors. Note that model formula contains both a date feature and derivatives of date - ARIMA uses the date - XGBoost uses the derivatives of date as regressors\n\nmodel_fit_arima_boosted &lt;- arima_boost(\n    min_n = 2,\n    learn_rate = 0.015\n) |&gt;\n    set_engine(engine = \"auto_arima_xgboost\") |&gt;\n    fit(value ~ date + \n          as.numeric(date) + \n          factor(month(date, \n                       label = TRUE), \n                 ordered = F),\n        data = training(splits))\n\nfrequency = 12 observations per 1 year\n\n\n\n\n1.2.3 Model 3: Exponential Smoothing (Modeltime)\nNext, create an Error-Trend-Season (ETS) model using an Exponential Smoothing State Space model. This is accomplished with exp_smoothing().\n\nmodel_fit_ets &lt;- exp_smoothing() |&gt;\n    set_engine(engine = \"ets\") |&gt;\n    fit(value ~ date, data = training(splits))\n\nfrequency = 12 observations per 1 year\n\n\n\n\n1.2.4 Model 4: Prophet (Modeltime)\nWe’ll create a prophet model using prophet_reg().\n\nmodel_fit_prophet &lt;- prophet_reg() |&gt;\n    set_engine(engine = \"prophet\") |&gt;\n    fit(value ~ date, data = training(splits))\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\n\n\n1.2.5 Model 5: Linear Regression (Parsnip)\nWe can model time series linear regression (TSLM) using the linear_reg() algorithm from parsnip. The following derivatives of date are used:\n\nTrend: Modeled using as.numeric(date)\nSeasonal: Modeled using month(date)\n\n\n# Model 5: lm ----\nmodel_fit_lm &lt;- linear_reg() |&gt;\n    set_engine(\"lm\") |&gt;\n    fit(value ~ as.numeric(date) + factor(month(date, label = TRUE), ordered = FALSE),\n        data = training(splits))\n\n\n\n1.2.6 Model 6: MARS (Workflow)\nWe can model a Multivariate Adaptive Regression Spline model using mars(). I’ve modified the process to use a workflow to standardize the preprocessing of the features that are provided to the machine learning model (mars).\n\nmodel_spec_mars &lt;- mars(mode = \"regression\") |&gt;\n    set_engine(\"earth\") \n\nrecipe_spec &lt;- recipe(value ~ date, data = training(splits)) |&gt;\n    step_date(date, features = \"month\", ordinal = FALSE) |&gt;\n    step_mutate(date_num = as.numeric(date)) |&gt;\n    step_normalize(date_num) |&gt;\n    step_rm(date)\n  \nwflw_fit_mars &lt;- workflow() |&gt;\n    add_recipe(recipe_spec) |&gt;\n    add_model(model_spec_mars) |&gt;\n    fit(training(splits))\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nLoading required package: plotrix\n\n\n\nAttaching package: 'plotrix'\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nOK, with these 6 models, we’ll show how easy it is to forecast.\n\n\n\n1.3 Step 3 - Add fitted models to a Model Table.\nThe next step is to add each of the models to a Modeltime Table using modeltime_table(). This step does some basic checking to make sure each of the models are fitted and that organizes into a scalable structure called a “Modeltime Table” that is used as part of our forecasting workflow.\n:::: {.columns} ::: {.column width=“50%”}\n\nmodels_tbl &lt;- modeltime_table(\n    model_fit_arima_no_boost,\n    model_fit_arima_boosted,\n    model_fit_ets,\n    model_fit_prophet,\n    model_fit_lm,\n    wflw_fit_mars\n)\n\n::: ::: {.column width=“50%”}\n\nmodels_tbl\n\n# Modeltime Table\n# A tibble: 6 × 3\n  .model_id .model     .model_desc                              \n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                                    \n1         1 &lt;fit[+]&gt;   ARIMA(0,1,1)(0,1,1)[12]                  \n2         2 &lt;fit[+]&gt;   ARIMA(0,1,1)(0,1,1)[12] W/ XGBOOST ERRORS\n3         3 &lt;fit[+]&gt;   ETS(M,A,A)                               \n4         4 &lt;fit[+]&gt;   PROPHET                                  \n5         5 &lt;fit[+]&gt;   LM                                       \n6         6 &lt;workflow&gt; EARTH                                    \n\n\n::: ### Step 4 - Calibrate the model to a testing set.\nCalibrating adds a new column, .calibration_data, with the test predictions and residuals inside.\n\ncalibration_tbl &lt;- models_tbl |&gt;\n    modeltime_calibrate(new_data = testing(splits))\n\ncalibration_tbl\n\n# Modeltime Table\n# A tibble: 6 × 5\n  .model_id .model     .model_desc                       .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                             &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;fit[+]&gt;   ARIMA(0,1,1)(0,1,1)[12]           Test  &lt;tibble [31 × 4]&gt;\n2         2 &lt;fit[+]&gt;   ARIMA(0,1,1)(0,1,1)[12] W/ XGBOO… Test  &lt;tibble [31 × 4]&gt;\n3         3 &lt;fit[+]&gt;   ETS(M,A,A)                        Test  &lt;tibble [31 × 4]&gt;\n4         4 &lt;fit[+]&gt;   PROPHET                           Test  &lt;tibble [31 × 4]&gt;\n5         5 &lt;fit[+]&gt;   LM                                Test  &lt;tibble [31 × 4]&gt;\n6         6 &lt;workflow&gt; EARTH                             Test  &lt;tibble [31 × 4]&gt;\n\n\n\n\n1.4 Step 5 - Testing Set Forecast & Accuracy Evaluation\nThere are 2 critical parts to an evaluation.\n\nVisualizing the Forecast vs Test Data Set\nEvaluating the Test (Out of Sample) Accuracy\n\n\n1.4.1 5A - Visualizing the Forecast Test\n\ncalibration_tbl |&gt;\n    modeltime_forecast(\n        new_data    = testing(splits),\n        actual_data = m750\n    ) |&gt;\n    plot_modeltime_forecast()\n\n\n\n\n\nFrom visualizing the test set forecast:\n\nModels 1&2: ARIMA & ARIMA Boost are performing well. Both models have “auto” components because we used Auto ARIMA. The XGBoost component has parameters that were specified. We can possibly get better accuracy by tuning, but because the ARIMA component is working well on this data, additional improvement may be low.\nModel 3: ETS(M,A,A) is performing the best. The 80% confidence interval is the most narrow of the bunch, indicating the hold out set is modeled well.\nModel 4: PROPHET is comparable to the ARIMA models, but has a slightly wider test error confidence interval.\nModel 5: LM is over-shooting the local trend. This is because the trend component is a simple linear line, which doesn’t account for the change points.\nModel 6: EARTH is overfitting the local trend. This is because we did not tune the number of change points, so the algorithm is auto-calculating the change points.\n\n\n\n1.4.2 5B - Accuracy Metrics\nWe can use modeltime_accuracy() to collect common accuracy metrics. The default reports the following metrics using yardstick functions:\n\nMAE - Mean absolute error, mae()\nMAPE - Mean absolute percentage error, mape()\nMASE - Mean absolute scaled error, mase()\nSMAPE - Symmetric mean absolute percentage error, smape()\nRMSE - Root mean squared error, rmse()\nRSQ - R-squared, rsq()\n\nThese of course can be customized following the rules for creating new yardstick metrics, but the defaults are very useful. Refer to default_forecast_accuracy_metrics() to learn more.\nTo make table-creation a bit easier, I’ve included table_modeltime_accuracy() for outputing results in either interactive (reactable) or static (gt) tables.\n\ncalibration_tbl |&gt;\n    modeltime_accuracy() |&gt;\n    table_modeltime_accuracy()\n\n\n\n\n\n\nFrom the accuracy metrics:\n\nModel 3: ETS is clearly the winner here with MAE of 77\nModel 6: MARS is over-fitting the local trend. This comes out in the R-Squared of 0.55.\n\n\n\n\n1.5 Step 6 - Refit to Full Dataset & Forecast Forward\nThe final step is to refit the models to the full dataset using modeltime_refit() and forecast them forward.\n\nrefit_tbl &lt;- calibration_tbl |&gt;\n    modeltime_refit(data = m750)\n\nrefit_tbl |&gt;\n    modeltime_forecast(h = \"3 years\", actual_data = m750) |&gt;\n    plot_modeltime_forecast()"
  },
  {
    "objectID": "4_Model.html#refitting---what-happened",
    "href": "4_Model.html#refitting---what-happened",
    "title": "Workflow: Modeltime",
    "section": "2 Refitting - What happened?",
    "text": "2 Refitting - What happened?\nThe models have all changed! (Yes - this is the point of refitting)\n\nThe LM model looks much better now because the linear trend line has now been fit to new data that follows the longer term trend.\nThe EARTH model has a trend that is more representative of the near-term trend.\nThe PROPHET model has a trend that is very similar to the EARTH model (this is because both modeling algorithms use change points to model trend, and prophet’s auto algorithm seems to be doing a better job at adapting).\nThe ETS model has changed from (M,A,A) to (A,A,A).\nThe ARIMA model have been updated and better capture the upswing.\n\nThis is the (potential) benefit of refitting.\nMore often than not refitting is a good idea. Refitting:\n\nRetrieves your model and preprocessing steps\nRefits the model to the new data\nRecalculates any automations. This includes:\n\nRecalculating the long-term trend for Linear Model\nRecalculating the changepoints for the Earth Model\nRecalculating the ARIMA and ETS parameters\n\nPreserves any parameter selections. This includes:\n\nXGBoost Parameters in the Boosted ARIMA min_n = 2, learn_rate = 0.015.\nAny other defaults that are not automatic calculations are used.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis section of the workshop is adapted from the Modeltime resources provided for the modeltime package (Getting started with Modeltime)"
  },
  {
    "objectID": "2_EDA.html",
    "href": "2_EDA.html",
    "title": "Handling Time series datasets",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "2_EDA.html#introduction",
    "href": "2_EDA.html#introduction",
    "title": "Handling Time series datasets",
    "section": "1 Introduction",
    "text": "1 Introduction\nOnce the data has been collected, based on data science principles, it is recommended to undertake an exploratory data analysis for better understanding of the dataset and its characteristics. In time series analysis and forecasting projects, the same is applied as visualizations and extraction of time series features. We shall be dwelling on these concepts using the datasets loaded for the workshop till now."
  },
  {
    "objectID": "2_EDA.html#first-things-first-ensure-that-you-have-a-time-series-dataset.",
    "href": "2_EDA.html#first-things-first-ensure-that-you-have-a-time-series-dataset.",
    "title": "Handling Time series datasets",
    "section": "2 First things first: Ensure that you have a time series dataset.",
    "text": "2 First things first: Ensure that you have a time series dataset.\nIn many occasions, though the data collection has been carried out for a time series analysis, the capture of the timeline may not be adequate to conduct a time series analysis. The dataset may be imported into the analysis environment as a spreadsheet or a dataframe. Therefore, at the outset, we should look at the variable containing timeline and ensure that the dataset is a time series data.\n\n2.1 Understanding the time stamps.\nIn any time series data project, it is critical to understand whether the data has been collected as a regular or irregular time series data. Further, depending upon the research question and need of the study, a common time stamp is required for all the datasets for creating forecasting models in subsequent phases of analysis and interpretation. Let’s look at the data sets and understand it further.\n\n\n\n\n2.1.1 Time Series Air Quality Data of Manali (2010-2023)\nWe shall be using the function glimpse() which makes it possible to see every column in a data frame and understand the class of the variables.\n\n\n\ndf_aqi |&gt; \n  glimpse()\n\n\n\n\nRows: 458\nColumns: 18\n$ `Regional Office Lab` &lt;chr&gt; \"RO-Kullu\", \"RO-Kullu\", \"RO-Kullu\", \"RO-Kullu\", …\n$ City                  &lt;chr&gt; \"Manali\", \"Manali\", \"Manali\", \"Manali\", \"Manali\"…\n$ `Station Name`        &lt;chr&gt; \"Manali-I\", \"Manali-I\", \"Manali-I\", \"Manali-I\", …\n$ `Sample Date`         &lt;dttm&gt; 2020-04-08, 2020-04-10, 2020-04-20, 2020-04-22,…\n$ `PM10 (µg/m³)`        &lt;dbl&gt; 12.31, 14.87, 9.45, 8.89, 9.09, 11.93, 11.04, 16…\n$ `PM2.5 (µg/m³)`       &lt;dbl&gt; 8.31, 6.23, 4.25, 6.24, 5.00, 8.33, 8.39, 8.39, …\n$ `SO₂ (µg/m³)`         &lt;dbl&gt; 1.08, 1.20, 1.09, 1.21, 1.13, 1.47, 1.62, 1.50, …\n$ `NOₓ (µg/m³)`         &lt;dbl&gt; 3.62, 4.19, 3.78, 4.00, 3.91, 4.31, 4.94, 4.57, …\n$ `NH₃ (µg/m³)`         &lt;dbl&gt; 2.16, 1.63, 1.64, 1.38, 1.62, 1.19, 1.84, 1.70, …\n$ `O₃ (µg/m³)`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pb (µg/m³)`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `CO (mg/m³)`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `C₆H₆ (µg/m³)`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `BaP (ng/m³)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `As (ng/m³)`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ni (ng/m³)`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ AQI                   &lt;chr&gt; \"14\", \"15\", \"9\", \"10\", \"9\", \"14\", \"14\", \"16\", \"2…\n$ `AQI Condition`       &lt;chr&gt; \"GOOD\", \"GOOD\", \"GOOD\", \"GOOD\", \"GOOD\", \"GOOD\", …\n\n\n\n\nLooking at the class, it is understood that the Sample Date in the data is stored as a dttm or date time variable. Also, PM10 (µg/m³) and other air quality variables are stored as character variables and not as numeric. Therefore, we shall use ymd() function from the package lubridate and tidyverse functions to create a time variable and convert character to numeric variable.\n\n\n\n\n\n\n\n\nNote\n\n\n\nlubridate package from tidyverse is a powerful tool for handling dates and time in a dataset.\n\n\n\ndf_aqi &lt;- df_aqi |&gt; \n  mutate(`Sample Date` = ymd(`Sample Date`)) |&gt; \n  mutate(`PM10 (µg/m³)` = as.numeric(`PM10 (µg/m³)`))\n\nLet us now determine the timestamp for the dataset\n\n\n\ndf_aqi |&gt; \n  select(`Sample Date`, `PM10 (µg/m³)`) |&gt; \n  head()\n\n\n\n\n  Sample Date PM10 (µg/m³)\n1  2020-04-08        12.31\n2  2020-04-10        14.87\n3  2020-04-20         9.45\n4  2020-04-22         8.89\n5  2020-04-24         9.09\n6  2020-05-04        11.93\n\n\n\n\n\ndf_aqi &lt;- df_aqi |&gt; \n  select(`Sample Date`, `PM10 (µg/m³)`)\n\nIt is understood that the sampling dates are not on daily time stamps and are irregular in nature. therefore, to further proceed, we shall be converting them to weekly time stamps.\n\n\n\ndf_aqi &lt;- df_aqi |&gt; \n  mutate(`Sample Date` = yearmonth(`Sample Date`)) |&gt; \n  group_by(`Sample Date`) |&gt; \n  summarise(`PM10 (µg/m³)` = mean(`PM10 (µg/m³)`, \n                                  na.rm = T))\n\n\n\n\n# A tibble: 6 × 2\n  `Sample Date` `PM10 (µg/m³)`\n          &lt;mth&gt;          &lt;dbl&gt;\n1      2020 Apr           10.9\n2      2020 May           19.8\n3      2020 Jun           33.1\n4      2020 Jul           33.5\n5      2020 Aug           26.2\n6      2020 Sep           43.0\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nChoosing a time stamp is of utmost importance in time series analysis when dealing with irregular time series data. It is advisable to use a parsimonious approach based on both visualizations and statistical measures for noise and entropy for decision making."
  },
  {
    "objectID": "2_EDA.html#creating-time-series-objects.",
    "href": "2_EDA.html#creating-time-series-objects.",
    "title": "Handling Time series datasets",
    "section": "3 Creating time series objects.",
    "text": "3 Creating time series objects.\n\n\n\n\n\n\n\n\nThe index variable.\n\n\n\ntsibble objects extend tidy data frames (tibble objects) by introducing temporal structure. We have set the time series index to be the Year column, which associates the measurements with the time of recording.\n\n\nNow, since we are aware of the timestamps, the next step is to create tsibble objects for extracting the time series characteristics from a time series dataset. A time series can be thought of as a list of numbers (the measurements), along with some information about what times those numbers were recorded (the index). This information can be stored as a tsibble object in R. We shall be using tsibble function from the package tsibble for the same.\n\n\n\ndf_disaster &lt;- df_disaster |&gt; \n  tsibble(\n  index = Year\n)\n\n\n\n\n# A tsibble: 6 x 2 [1Y]\n   Year deaths_earthquakes\n  &lt;dbl&gt;              &lt;dbl&gt;\n1  1900                560\n2  1901                 72\n3  1902              31944\n4  1903              26520\n5  1904                612\n6  1905              83473\n\n\n\n\nIn scenarios wherein the time stamps are quarterly, monthly, or weekly, respective index functions are used.\n\n\n\n\n\n\n\n\nThe key variable\n\n\n\nA tsibble also allows multiple time series to be stored in a single object. The details of the same are stored in key variable. For example, time series data of men and women.\n\n\n\n\n\ndf_rta &lt;- df_rta |&gt; \n  mutate(timeline = dmy(timeline)) |&gt; \n  mutate(Month = yearmonth(timeline)) |&gt; \n  group_by(Month) |&gt; \n  summarise(accidents = sum(`Total Accident`)) |&gt; \n  tsibble(\n  index =  Month\n)\n\n\n\n\n# A tsibble: 6 x 2 [1M]\n     Month accidents\n     &lt;mth&gt;     &lt;int&gt;\n1 2014 Jan     41954\n2 2014 Feb     39899\n3 2014 Mar     42524\n4 2014 Apr     39867\n5 2014 May     45404\n6 2014 Jun     42448"
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction to time series analysis",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "1_intro.html#unveiling-the-dynamics-of-time-an-introduction-to-time-series-data-analysis-and-forecasting.",
    "href": "1_intro.html#unveiling-the-dynamics-of-time-an-introduction-to-time-series-data-analysis-and-forecasting.",
    "title": "Introduction to time series analysis",
    "section": "1 Unveiling the Dynamics of Time: An Introduction to Time Series Data Analysis and Forecasting.",
    "text": "1 Unveiling the Dynamics of Time: An Introduction to Time Series Data Analysis and Forecasting.\nIn the ever-evolving landscape of data science, understanding the intricacies of time series data has become increasingly crucial. As we embark on a journey into the realm of temporal patterns and sequential dependencies, this workshop aims to demystify the concepts and methodologies that constitute time series analysis and forecasting. Time series data, a distinctive form of information, holds the key to unraveling trends, forecasting future events, and gaining profound insights into diverse fields such as finance, economics, climate science, and beyond. So, lets dive in!!."
  },
  {
    "objectID": "1_intro.html#what-is-time-series-data",
    "href": "1_intro.html#what-is-time-series-data",
    "title": "Introduction to time series analysis",
    "section": "2 What is time series data?",
    "text": "2 What is time series data?\nTime series data is a type of data that is collected or recorded over a sequence of time intervals. Time series data is one of the most common formats of data, and it is used to describe an event or phenomena that occurs over time. Time series data has a simple requirement—its values need to be captured at equally spaced time intervals, such as seconds, minutes, hours, days, months, and so on. This important characteristic is one of the main attributes of the series and is known as the frequency of the series. We usually add the frequency along with the name of the series. In healthcare, some of the examples of time series data include:\n\nDaily patient records\nHourly test results\nMonthly disease incidence rates\nDaily air quality indicators\nWeekly admissions to an emergency department\nAnnual expenditures on health care\n\nA univariate time series is a sequence of measurements of the same variable collected over time.\n\n\n\n\n\n\n\n\nThink! How data collection change your analysis strategy?\n\n\n\nHow would you collect data regarding hospitalizations? Would you like to have data wherein the last hospitalization record is maintained and updated or would you like to have a single data entry point for each hospitalization?"
  },
  {
    "objectID": "1_intro.html#why-time-series-analysis-and-forecasting",
    "href": "1_intro.html#why-time-series-analysis-and-forecasting",
    "title": "Introduction to time series analysis",
    "section": "3 Why time series analysis and forecasting?",
    "text": "3 Why time series analysis and forecasting?\n\n3.1 Time series analysis.\nTime series analysis is the art of extracting meaningful insights and revealing patterns from time series data using statistical and data visualization approaches. These insights and patterns can then be utilized to explore past events and forecast future values in the series. There are three different aims of Time series Analysis:-\n\nDescriptive analysis.\nExplanatory analysis.\nForecasting.\n\n\n\n3.2 Forecasting\nForecasting in healthcare is crucial for various aspects of planning, resource allocation, and managing patient care efficiently. Here are some examples where forecasting plays a vital role:\n\nDisease Outbreaks and Epidemic Prediction: Forecasting the spread of infectious diseases, such as influenza or COVID-19, helps in preparing healthcare systems for potential surges in cases. It aids in vaccine distribution, setting up quarantine measures, and ensuring sufficient medical supplies and staff are available.\nPatient Admission Rates: Hospitals and clinics use forecasting to predict patient admission rates. This helps in staffing decisions, ensuring there are enough healthcare professionals on duty to meet demand, and in planning bed occupancy rates to optimize the use of available resources.\nPharmaceutical Supply Chain: Forecasting is used to predict the demand for various medications, helping pharmacies and hospitals maintain an optimal inventory. This is crucial for managing costs, reducing waste, and ensuring that essential medicines are always in stock, especially for chronic conditions or in emergency situations.\nSurgical and Procedure Needs: By predicting the demand for certain types of surgeries or medical procedures, healthcare providers can better schedule operating rooms, allocate medical staff, and ensure the necessary equipment and supplies are available, thereby improving patient care and operational efficiency.\nStaffing Requirements: Forecasting helps predict staffing needs based on various factors, including seasonal trends in illnesses, epidemic outbreaks, or changing demographics. This ensures that there are enough healthcare workers, including doctors, nurses, and support staff, to provide quality care without overburdening the existing workforce.\nHealthcare Policy and Infrastructure Planning: Long-term forecasts are used by policymakers to plan healthcare infrastructure, such as the construction of new hospitals or clinics, expansion of existing facilities, and investment in new technologies. These forecasts consider population growth, aging, and changes in disease prevalence.\nPreventive Care Needs: By forecasting trends in various diseases or health conditions, healthcare providers can plan and implement preventive care measures more effectively. This could include vaccination campaigns, public health initiatives, or screening programs aimed at early detection of conditions like cancer, diabetes, or heart disease.\n\nForecasting in healthcare utilizes a variety of data sources, including historical health data, demographic trends, environmental factors, and epidemiological models. The goal is to make informed decisions that improve patient care, enhance operational efficiency, and effectively manage resources. However, it is important to understand that the predictability of an event or a quantity depends on several factors including:\n\nHow well we understand the factors that contribute to it;\nHow much data is available;\nHow similar the future is to the past;\nWhether the forecasts can affect the thing we are trying to forecast."
  },
  {
    "objectID": "1_intro.html#datasets-used-in-the-workshop.",
    "href": "1_intro.html#datasets-used-in-the-workshop.",
    "title": "Introduction to time series analysis",
    "section": "4 Datasets used in the workshop.",
    "text": "4 Datasets used in the workshop.\n\n4.1 Time Series Air Quality Data of Manali (2010-2023)\n\n\n\n\n\nTable 1:  Overview: Air Quality Regional Office LabCityStation NameSample DatePM10 (µg/m³)PM2.5 (µg/m³)SO₂ (µg/m³)NOₓ (µg/m³)NH₃ (µg/m³)O₃ (µg/m³)Pb (µg/m³)CO (mg/m³)C₆H₆ (µg/m³)BaP (ng/m³)As (ng/m³)Ni (ng/m³)AQIAQI ConditionRO-KulluManaliManali-I2020-04-08 00:00:0012.318.311.083.622.1614GOODRO-KulluManaliManali-I2021-01-13 00:00:0048.862.0011.243.0649GOOD\n\n\nThe dataset shared for the workshop (Table 1) is a subset of air quality data from SPCB website. The shared dataset includes data for one randomly choosen station in Manali, HP. The dataset includes data on PM 10, Pm 2.5, AQI and multiple other parameters from the meterological station.\n\ndf_aqi &lt;- rio::import(here::here(\"data\",\n                                 \"aqidata_IPHACON2024.xlsx\"))\n\n\n\n4.2 Global Deaths from Earthquakes (1900-2023)\n\n\n\n\n\nTable 2:  Overview: Earthquake deaths Yeardeaths_earthquakes1,9005601,920731,9921,99987,6192,023218,608\n\n\nAs shown in Table 2 the dataset used in the workshop is created to show global number of deaths due to earthquakes. Poverty, disease, hunger, climate change, war, existential risks, and inequality: The world faces many great and terrifying problems. It is these large problems that the work at Our World in Data focuses on. Our World in Data’s mission is to publish the “research and data to make progress against the world’s largest problems”.\n\ndf_disaster &lt;- rio::import(here::here(\"data\",\n                       \"earthquakes_IPHACON2024.xlsx\"))\n\n\n\n4.3 Monthly Road Traffic accidents in India (2014-2018)\n\n\n\n\n\nTable 3:  Overview: RTAs in India timelineTotal Accident01/04/201439,86701/08/201835,84501/11/201838,41701/09/201835,387\n\n\nAs shown in Table 3 the dataset used in the workshop is subset of data taken from https://data.gov.in/. It has the number of road accidents from 2014-2018 in India.\n\ndf_rta &lt;- rio::import(here::here(\"data\",\n                       \"RTA_monthly_India_IPHACON2024.csv\"))\n\n\n\n4.4 Maternal Mortality Ratio: India (2000-2020)\n\n\n\n\n\nTable 4:  Overview: Reporting YearMMR2,0003842,0043012,020103\n\n\nAs shown in Table 4 the dataset used in the workshop is Maternal mortality ratio is the number of women who die from pregnancy-related causes while pregnant or within 42 days of pregnancy termination per 100,000 live births. The data are estimated with a regression model using information on the proportion of maternal deaths among non-AIDS deaths in women ages 15-49, fertility, birth attendants, and GDP measured using purchasing power parities (PPPs). The source of the dataset is WHO, UNICEF, UNFPA, World Bank Group, and UNDESA/Population Division. Trends in Maternal Mortality 2000 to 2020. Geneva, World Health Organization, 2023\n\ndf_mmr &lt;- rio::import(here::here(\"data\",\n                       \"mmr_IPHACON2024.xls\"))\n\n\n\n4.5 Campylobacter cases in Germany (2001-2011)\n\n\n\n\n\nTable 5:  Overview: Campylobacter cases datecase2001-12-31 00:00:005142002-01-28 00:00:008152002-05-20 00:00:00869\n\n\nAs shown in Table 5 the dataset used in the workshop is the counts of campylobacter cases reported in Germany between 2001 and 2011. The dataset is obtained from the surveillance package\n\ndf_campylo &lt;- rio::import(here::here(\"data\",\n                       \"campylobacter_IPHACON2024.xlsx\"))\n\n\n\n4.6 M750 data\n\n\n\n\n\nTable 6:  Overview: M750 iddatevalueM7501990-01-01 00:00:006,370M7501994-02-01 00:00:007,240M7502006-08-01 00:00:008,580M7502015-06-01 00:00:0011,000\n\n\nThe Table 6 represents data from the fourth M Competition. M4, started on 1 January 2018 and ended in 31 May 2018. The competition included 100,000 time series datasets. This dataset includes The 750th Monthly Time Series used in the competition.\n\ndf_750 &lt;- rio::import(here::here(\"data\",\n                       \"m750_IPHACON2024.xlsx\"))\n\n\n\n4.7 Anti-Diabetes drug sales\n\n\n\n\n\nTable 7:  Overview: Anti-Diabetic Drug sales MonthTotalC1991 Jul3,526,5911995 Aug5,855,2772003 Dec16,503,9662008 Feb21,654,285\n\n\nThe Table 7 represents data from the fourth M Competition. M4, started on 1 January 2018 and ended in 31 May 2018. The competition included 100,000 time series datasets. This dataset includes The 750th Monthly Time Series used in the competition.\n\ndf_diab &lt;- PBS |&gt;\n  filter(ATC2 == \"A10\") |&gt;\n  select(Month, Concession, Type, Cost) |&gt;\n  summarise(TotalC = sum(Cost))"
  },
  {
    "objectID": "2.1_EDA.html",
    "href": "2.1_EDA.html",
    "title": "Exploratory Time Series Analysis",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "2.1_EDA.html#concept-of-stationarity-and-transformations.",
    "href": "2.1_EDA.html#concept-of-stationarity-and-transformations.",
    "title": "Exploratory Time Series Analysis",
    "section": "1 Concept of stationarity and transformations.",
    "text": "1 Concept of stationarity and transformations.\nIt is important to understand that ARIMA models are based on assumption that the time series data is stationary in nature. A time series is said to be stationary when the mean value of each time component is similar (i.e no trend or seasonality). In other words, A stationary time series is one whose statistical properties do not depend on the time at which the series is observed. However, often that is not the case. If the data shows variation that increases or decreases with the level of the series, then a transformation can be useful. For example, a logarithmic transformation is often useful to make a series stationary. Another useful family of transformations, that includes both logarithms and power transformations, is the family of Box-Cox transformations. Also, differencing of the order one or two is a commonly used approach for making time series stationary. We shall be discussing this topic further as we progress in the workshop for a better understanding."
  },
  {
    "objectID": "2.1_EDA.html#time-series-components.",
    "href": "2.1_EDA.html#time-series-components.",
    "title": "Exploratory Time Series Analysis",
    "section": "2 Time series components.",
    "text": "2 Time series components.\nA time series data is often described in terms of trend, seasonality and cyclicity.\n\nTrend. A trend is said to be present when there is a long-term increase or decrease in the data. This change does not always have to be linear.\nSeasonality. A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period.\nCyclicity. A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency."
  },
  {
    "objectID": "2.1_EDA.html#visualizations.",
    "href": "2.1_EDA.html#visualizations.",
    "title": "Exploratory Time Series Analysis",
    "section": "3 Visualizations.",
    "text": "3 Visualizations.\n\n3.1 Time plots\nWhat kind of trend, seasonality and cyclicity do you observe in the following visualizations?\n\n\n# Creating tsibble objects\n\ndf_aqi &lt;- df_aqi |&gt; \n  as_tsibble() \n\nUsing `Sample Date` as index variable.\n\ndf_rta &lt;- df_rta |&gt; \n  as_tsibble() \n\ndf_disaster &lt;- df_disaster |&gt; \n  as_tsibble() \n\ndf_mmr &lt;- df_mmr |&gt; \n  mutate(Year = dmy(\n    str_c(\"01-01-\", Year)\n  )) |&gt; \n  as_tsibble() \n\nUsing `Year` as index variable.\n\ndf_campylo &lt;- df_campylo |&gt; \n  mutate(date = yearweek(date)) |&gt; \n  as_tsibble() \n\nUsing `date` as index variable.\n\ndf_750 &lt;- df_750 |&gt; \n  mutate(date = yearmonth(date)) |&gt; \n  as_tsibble() \n\nUsing `date` as index variable.\n\n\n\nCode\ndf_aqi |&gt; \n  autoplot()\ndf_rta |&gt; \n  autoplot()\ndf_disaster |&gt; \n  autoplot()\ndf_mmr |&gt; \n  autoplot()\ndf_campylo |&gt; \n  autoplot()\ndf_750 |&gt; \n  autoplot()\ndf_diab |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n(a) AQI\n\n\n\n\n\n\n\n(b) RTA\n\n\n\n\n\n\n\n(c) Earthquakes\n\n\n\n\n\n\n\n\n\n(d) MMR\n\n\n\n\n\n\n\n(e) Campylobacter\n\n\n\n\n\n\n\n(f) M750\n\n\n\n\n\n\n\n\n\n(g) Diabetes drug sales\n\n\n\n\nFigure 1: Visualizations for Trend, seasonality and cyclicity\n\n\n\n\n\n3.2 Seasonal plots.\nA seasonal plot is similar to a time plot except that the data are plotted against the individual “seasons” in which the data were observed.A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and is especially useful in identifying years in which the pattern changes.\n\n\n\ndf_diab |&gt;\n  gg_season(TotalC, labels = \"both\") +\n  labs(y = \"$ (millions)\",\n       title = \"Seasonal plot: Antidiabetic drug sales\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 Seasonal subseries plot.\nSeasonal subseries plot is an alternative plot that emphasises the seasonal patterns is where the data for each season are collected together in separate mini time plots. The blue horizontal lines indicate the means for each month. This form of plot enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons.\n\n\n\ndf_diab |&gt;\n  gg_subseries(TotalC) +\n  labs(\n    y = \"$ (millions)\",\n    title = \"Antidiabetic drug sales\"\n  )"
  },
  {
    "objectID": "2.1_EDA.html#time-series-features.",
    "href": "2.1_EDA.html#time-series-features.",
    "title": "Exploratory Time Series Analysis",
    "section": "4 Time series features.",
    "text": "4 Time series features.\n\n4.1 Autocorrelation function (ACF)\nJust as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. In a time series, the autocorrelation coefficients between varied lags make up the autocorrelation function or ACF.\n\n\n\ndf_aqi |&gt; \n  ACF(`PM10 (µg/m³)`, \n      lag_max = 5)\n\n\n\n\n# A tsibble: 5 x 2 [1M]\n       lag   acf\n  &lt;cf_lag&gt; &lt;dbl&gt;\n1       1M 0.447\n2       2M 0.116\n3       3M 0.247\n4       4M 0.244\n5       5M 0.150\n\n\n\n\nThe same can also be visualized through ACF plot for better understanding.\n\n\nCode\ndf_aqi |&gt; \n  ACF(`PM10 (µg/m³)`) |&gt; \n  autoplot()\ndf_rta |&gt; \n  ACF(accidents) |&gt; \n  autoplot()\ndf_disaster |&gt; \n  ACF(deaths_earthquakes) |&gt; \n  autoplot()\ndf_campylo |&gt; \n  ACF(case) |&gt; \n  autoplot()\ndf_750 |&gt; \n  ACF(value) |&gt; \n  autoplot()\ndf_diab |&gt; \n  ACF(TotalC) |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n(a) AQI\n\n\n\n\n\n\n\n(b) RTA\n\n\n\n\n\n\n\n(c) Earthquakes\n\n\n\n\n\n\n\n\n\n(d) MMR\n\n\n\n\n\n\n\n(e) Campylobacter\n\n\n\n\n\n\n\n(f) M750\n\n\n\n\nFigure 2: Visualizations for Trend, seasonality and cyclicity\n\n\n\n\n\n4.2 Additional ACF features\n\n\nThe feat_acf() function computes a selection of the autocorrelations as under:\n\nThe first autocorrelation coefficient from the original data; the sum of squares of the first ten autocorrelation coefficients from the original data;\nThe first autocorrelation coefficient from the differenced data;\nThe sum of squares of the first ten autocorrelation coefficients from the differenced data;\nThe first autocorrelation coefficient from the twice differenced data;\nThe sum of squares of the first ten autocorrelation coefficients from the twice differenced data;\nFor seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned.\n\n\n\n\nCode\ndf_aqi |&gt;\n  features(`PM10 (µg/m³)`,\n           feat_acf)|&gt; \n  pivot_longer(cols = everything())\n\n\n# A tibble: 7 × 2\n  name          value\n  &lt;chr&gt;         &lt;dbl&gt;\n1 acf1         0.447 \n2 acf10        0.395 \n3 diff1_acf1  -0.121 \n4 diff1_acf10  0.540 \n5 diff2_acf1  -0.318 \n6 diff2_acf10  0.599 \n7 season_acf1 -0.0140"
  },
  {
    "objectID": "2.1_EDA.html#time-series-decomposition",
    "href": "2.1_EDA.html#time-series-decomposition",
    "title": "Exploratory Time Series Analysis",
    "section": "5 Time series decomposition",
    "text": "5 Time series decomposition\n\n\n\n\n\n\n\n\nDecomposition methods\n\n\n\nThe additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate.\n\n\nTime series data is decomposed into trend, seasonality and remainder components based on additive or multiplicative decomposition methods. If we assume an additive decomposition, then we can write \\(y_t = S_t + T_t + R_t\\) where \\(y_t\\) is the data, \\(S_t\\) is the seasonal component, \\(T_t\\) is the trend-cycle component, and \\(R_t\\) is the remainder component, all at period \\(t\\). Alternatively, a multiplicative decomposition would be written as \\(y_t = S_t * T_t * R_t\\).\n\n\n\ndf_diab |&gt;\n  model(stl = STL(TotalC)) |&gt; \n  components()\n\n# A dable: 204 x 7 [1M]\n# Key:     .model [1]\n# :        TotalC = trend + season_year + remainder\n   .model    Month  TotalC    trend season_year remainder season_adjust\n   &lt;chr&gt;     &lt;mth&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 stl    1991 Jul 3526591 3270635.    -127065.   383021.      3653656.\n 2 stl    1991 Aug 3180891 3310236.    -116422.   -12923.      3297313.\n 3 stl    1991 Sep 3252221 3349837.     -87179.   -10437.      3339400.\n 4 stl    1991 Oct 3611003 3389438.     134369.    87196.      3476634.\n 5 stl    1991 Nov 3565869 3429518.     254173.  -117822.      3311696.\n 6 stl    1991 Dec 4306371 3469598.    1640931.  -804158.      2665440.\n 7 stl    1992 Jan 5088335 3509677.    1811795.  -233137.      3276540.\n 8 stl    1992 Feb 2814520 3549836.   -1143268.   407952.      3957788.\n 9 stl    1992 Mar 2985811 3589994.    -704096.    99913.      3689907.\n10 stl    1992 Apr 3204780 3630153.    -727385.   302012.      3932165.\n# ℹ 194 more rows\n\n\n\n\ndf_diab |&gt;\n  model(stl = STL(TotalC)) |&gt; \n  components()|&gt;\n  autoplot()\n\n\n\n\n\n\n\n5.1 Additional STL features\n\n\nA time series decomposition can be used to measure the strength of trend and seasonality in a time series. The feat_stl() function returns several more features described as under:\n\nseasonal_peak_year indicates the timing of the peaks — which month or quarter contains the largest seasonal component. - seasonal_trough_year indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.\nspikiness measures the prevalence of spikes in the remainder component\n\nlinearity measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.\ncurvature measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from a quadratic regression applied to the trend component.\nstl_e_acf1 is the first autocorrelation coefficient of the remainder series.\nstl_e_acf10 is the sum of squares of the first ten autocorrelation coefficients of the remainder series.\n\n\n\ndf_diab |&gt;\n  features(TotalC, \n           feat_stl) |&gt; \n  pivot_longer(cols = everything())\n\n# A tibble: 9 × 2\n  name                       value\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 trend_strength          9.88e- 1\n2 seasonal_strength_year  8.77e- 1\n3 seasonal_peak_year      7   e+ 0\n4 seasonal_trough_year    8   e+ 0\n5 spikiness               2.99e+19\n6 linearity               7.88e+ 7\n7 curvature               1.81e+ 7\n8 stl_e_acf1             -2.22e- 1\n9 stl_e_acf10             1.48e- 1"
  },
  {
    "objectID": "2.1_EDA.html#moving-averages.",
    "href": "2.1_EDA.html#moving-averages.",
    "title": "Exploratory Time Series Analysis",
    "section": "6 Moving averages.",
    "text": "6 Moving averages.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSimple moving averages such as these are usually of an odd order. the middle observation, and \\(k\\) observations on either side, are averaged. In case of even, moving average to a moving average is applied to make an even-order moving average symmetric. Also, weighted MAs are used to yield a smoother estimate of the trend-cycle.\n\n\nThe classical method of time series decomposition originated in the 1920s and was widely used until the 1950s. It still forms the basis of many time series decomposition methods, so it is important to understand how it works. The first step in a classical decomposition is to use a moving average method to estimate the trend-cycle. A moving average of order \\(m\\) can be written as\n\\(T_t = 1/m \\sum_{j = -k}^k y_{t+j}\\) where \\(m = 2k+1\\)\nThe estimate of the trend-cycle at time \\(t\\) is obtained by averaging values of the time series within \\(k\\) periods of \\(t\\) . Observations that are nearby in time are also likely to be close in value. Therefore, the average eliminates some of the randomness in the data, leaving a smooth trend-cycle component. We call this an \\(m-MA\\), meaning a moving average of order \\(m\\).\n\n\n\ndiab_ma5 &lt;- df_diab |&gt;\n  mutate(\n    `5-MA` = slider::slide_dbl(TotalC, mean,\n                .before = 2, \n                .after = 2, \n                .complete = TRUE)\n  )\n\nTo see what the trend-cycle estimate looks like, we plot it along with the original data\n\n\n\n\n\n\nThink over!\n\n\n\nHow will you calculate moving averages for 3-MA, 7-MA, and 9-MA?\n\n\n\n\n\nCode\ndiab_ma3 |&gt;\n  autoplot(TotalC) +\n  geom_line(aes(y = `3-MA`), colour = \"red\") \ndiab_ma5 |&gt;\n  autoplot(TotalC) +\n  geom_line(aes(y = `5-MA`), colour = \"red\") \ndiab_ma7 |&gt;\n  autoplot(TotalC) +\n  geom_line(aes(y = `7-MA`), colour = \"red\") \ndiab_ma9 |&gt;\n  autoplot(TotalC) +\n  geom_line(aes(y = `9-MA`), colour = \"red\") \n\n\n\n\n\n\n\n\n(a) 3-MA\n\n\n\n\n\n\n\n(b) 5-MA\n\n\n\n\n\n\n\n\n\n(c) 7-MA\n\n\n\n\n\n\n\n(d) 9-MA\n\n\n\n\nFigure 3: Moving averages"
  },
  {
    "objectID": "3_AR_MA.html",
    "href": "3_AR_MA.html",
    "title": "ARIMA models",
    "section": "",
    "text": "Welcome to the world of Time Series Analysis and forecasting!!"
  },
  {
    "objectID": "3_AR_MA.html#introduction.",
    "href": "3_AR_MA.html#introduction.",
    "title": "ARIMA models",
    "section": "1 Introduction.",
    "text": "1 Introduction.\nExponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data."
  },
  {
    "objectID": "3_AR_MA.html#autoregressive-ar-models.",
    "href": "3_AR_MA.html#autoregressive-ar-models.",
    "title": "ARIMA models",
    "section": "2 Autoregressive (AR) models.",
    "text": "2 Autoregressive (AR) models.\nAutoregressive models are remarkably flexible at handling a wide range of different time series patterns. In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. Thus, an autoregressive model of order \\(p\\) can be written as\n\\(y_t = c + \\phi_1 y_{t-1}+ \\phi_2y_{t-2}+ \\phi_3y_{t-3}+.... \\phi_py_{t-p} + \\epsilon_t\\)"
  },
  {
    "objectID": "3_AR_MA.html#moving-average-ma-models.",
    "href": "3_AR_MA.html#moving-average-ma-models.",
    "title": "ARIMA models",
    "section": "3 Moving average (MA) models.",
    "text": "3 Moving average (MA) models.\nIn case of MA models, rather than using past values of the forecast variable in a regression, past forecast errors are used in a regression-like model\n\\(y_t = c + \\theta_1.\\epsilon_{t-1}+ \\theta_2.\\epsilon_{t-2}+ \\theta_3.\\epsilon_{t-3}+.... \\theta_p. \\epsilon_{t-p} + \\epsilon_t\\)"
  },
  {
    "objectID": "3_AR_MA.html#autoregressive-moving-average-models.",
    "href": "3_AR_MA.html#autoregressive-moving-average-models.",
    "title": "ARIMA models",
    "section": "4 Autoregressive Moving Average models.",
    "text": "4 Autoregressive Moving Average models.\n\n\n\n\n\n\n\n\nSARIMA models\n\n\n\nA seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA models and are represented as ARIMA(p,d,q)(P,D,Q)m.\n\n\n\n\nIf we combine differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for AutoRegressive Integrated Moving Average. This is called as ARIMA(\\(p,d,q\\)) model where \\(p\\) is the order of AR, \\(q\\) is the order of MA, and \\(q\\) is the order of differencing.\n\n\n\nSpecial cases of ARIMA models"
  }
]